/**
 * Phase 1 PoC: èªå½™è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
 * ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ100èªï¼‰ã‚’D1ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
 */

import { readFileSync } from 'fs';
import { writeFileSync } from 'fs';
import path from 'path';

interface VocabEntry {
  word_lemma: string;
  pos: string;
  cefr_level: string;
  zipf_score: number | null;
  grade_level: number | null;
  sources: string;
  confidence: number;
}

function parseCSV(csvContent: string): VocabEntry[] {
  const lines = csvContent.trim().split('\n');
  const headers = parseCSVLine(lines[0]);
  
  return lines.slice(1).map(line => {
    const values = parseCSVLine(line);
    const entry: any = {};
    
    headers.forEach((header, index) => {
      const value = values[index];
      
      if (header === 'zipf_score' || header === 'confidence') {
        entry[header] = value ? parseFloat(value) : null;
      } else if (header === 'grade_level') {
        entry[header] = value ? parseInt(value) : null;
      } else {
        entry[header] = value;
      }
    });
    
    return entry as VocabEntry;
  });
}

function parseCSVLine(line: string): string[] {
  const result: string[] = [];
  let current = '';
  let inQuotes = false;
  
  for (let i = 0; i < line.length; i++) {
    const char = line[i];
    
    if (char === '"') {
      inQuotes = !inQuotes;
    } else if (char === ',' && !inQuotes) {
      result.push(current.trim());
      current = '';
    } else {
      current += char;
    }
  }
  
  result.push(current.trim());
  return result;
}

async function importVocabularyData() {
  console.log('ğŸ“š Phase 1 PoC: Importing vocabulary data...\n');
  
  // CSVèª­ã¿è¾¼ã¿
  const csvPath = path.join(process.cwd(), 'data', 'vocabulary', 'sample_vocab_poc.csv');
  const csvContent = readFileSync(csvPath, 'utf-8');
  
  const records = parseCSV(csvContent);
  
  console.log(`âœ… Loaded ${records.length} vocabulary entries from CSV\n`);
  
  // D1ã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼‰
  // Wrangler D1 APIã‚’ä½¿ç”¨
  const insertStatements = records.map(entry => {
    // sourcesã‚’JSONé…åˆ—å½¢å¼ã«å¤‰æ›
    // CSVã‹ã‚‰ ["CEFR-J","NGSL","SVL"] ã®ã‚ˆã†ãªå½¢å¼ã«ãªã£ã¦ã„ã‚‹
    // SQLã«åŸ‹ã‚è¾¼ã‚€éš›ã¯ã€JSONæ–‡å­—åˆ—ã¨ã—ã¦ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ãŒå¿…è¦
    const sourcesJson = entry.sources.replace(/"/g, '""'); // SQLç”¨ã«ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    
    return `INSERT INTO eiken_vocabulary_lexicon 
      (word_lemma, pos, cefr_level, zipf_score, grade_level, sources, confidence)
      VALUES 
      ('${entry.word_lemma}', '${entry.pos}', '${entry.cefr_level}', 
       ${entry.zipf_score}, ${entry.grade_level}, '${sourcesJson}', ${entry.confidence});`;
  });
  
  // SQLç”Ÿæˆ
  const sqlPath = path.join(process.cwd(), 'data', 'vocabulary', 'import_sample_data.sql');
  const sqlContent = insertStatements.join('\n');
  
  writeFileSync(sqlPath, sqlContent);
  
  console.log(`âœ… Generated SQL file: ${sqlPath}`);
  console.log(`ğŸ“ Total INSERT statements: ${insertStatements.length}\n`);
  
  // CEFRåˆ†å¸ƒã‚’è¡¨ç¤º
  const cefrDistribution: Record<string, number> = {};
  records.forEach(entry => {
    cefrDistribution[entry.cefr_level] = (cefrDistribution[entry.cefr_level] || 0) + 1;
  });
  
  console.log('ğŸ“Š CEFR Distribution:');
  Object.entries(cefrDistribution)
    .sort()
    .forEach(([level, count]) => {
      const percentage = ((count / records.length) * 100).toFixed(1);
      console.log(`   ${level}: ${count} words (${percentage}%)`);
    });
  
  console.log('\nâœ… Import preparation complete!');
  console.log('   Next step: Run the following command to import:');
  console.log('   npx wrangler d1 execute kobeya-logs-db --local --file=./data/vocabulary/import_sample_data.sql');
}

importVocabularyData().catch(console.error);
