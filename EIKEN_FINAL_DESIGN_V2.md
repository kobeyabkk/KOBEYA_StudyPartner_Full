# è‹±æ¤œå¯¾ç­–ã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚è¨­è¨ˆæ›¸ V2.0ï¼ˆAIçµ±åˆãƒ¬ãƒ“ãƒ¥ãƒ¼åæ˜ ç‰ˆï¼‰

## ğŸš¨ é‡è¦ãªè¨­è¨ˆå¤‰æ›´ï¼ˆV1ã‹ã‚‰ã®æ”¹å–„ï¼‰

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯3ã¤ã®AIï¼ˆGensparkã€Claudeã€ChatGPTï¼‰ã‹ã‚‰ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµ±åˆã—ã€ä»¥ä¸‹ã®**é‡å¤§ãªå•é¡Œ**ã‚’è§£æ±ºã—ã¾ã™ï¼š

### âŒ V1ã®å•é¡Œç‚¹
1. **è‘—ä½œæ¨©ãƒªã‚¹ã‚¯ï¼ˆæœ€é‡è¦ï¼‰**: éå»å•ã®å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã‚’DBã«ç›´æ¥ä¿å­˜
2. **D1ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä¸è¶³**: ãƒãƒƒãƒå‡¦ç†æœªä½¿ç”¨ã€ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã®ãƒªã‚¹ã‚¯
3. **å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„æœªæœ‰åŠ¹åŒ–**: `PRAGMA foreign_keys = ON` ãŒä¸åœ¨
4. **Embeddingè¨ˆç®—ã‚³ã‚¹ãƒˆ**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§æ¯å›APIå‘¼ã³å‡ºã—
5. **Cronåˆ¶é™**: Workers CPUåˆ¶é™ï¼ˆ10ç§’ï¼‰ã‚’è¶…ãˆã‚‹é•·æ™‚é–“å‡¦ç†
6. **è‡ªå‹•æ›´æ–°ãƒˆãƒªã‚¬ãƒ¼ä¸åœ¨**: `updated_at` ã®æ‰‹å‹•æ›´æ–°å¿…é ˆ

### âœ… V2ã®è§£æ±ºç­–
1. **è‘—ä½œæ¨©å®‰å…¨è¨­è¨ˆ**: éå»å•ã¯åˆ†æçµæœã®ã¿ä¿å­˜ï¼ˆå•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã¯ä¿å­˜ã—ãªã„ï¼‰
2. **D1ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³**: `env.DB.batch()` ã«ã‚ˆã‚‹åŸå­æ€§ä¿è¨¼
3. **å¤–éƒ¨ã‚­ãƒ¼æœ‰åŠ¹åŒ–**: èµ·å‹•æ™‚ã« `PRAGMA foreign_keys = ON` å®Ÿè¡Œ
4. **Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥**: KV + D1ã®2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥
5. **Durable Objects**: é•·æ™‚é–“AIç”Ÿæˆã‚¿ã‚¹ã‚¯ç”¨
6. **è‡ªå‹•æ›´æ–°ãƒˆãƒªã‚¬ãƒ¼**: `updated_at` ã®è‡ªå‹•æ›´æ–°
7. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å±¤**: student_profilesã€audit_logsã€JWTèªè¨¼

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆï¼ˆè‘—ä½œæ¨©å®‰å…¨ç‰ˆï¼‰

### æ ¸å¿ƒåŸå‰‡
- âœ… **éå»å•ã¯åˆ†æçµæœã®ã¿**: å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã¯ä¿å­˜ã—ãªã„
- âœ… **AIç”Ÿæˆå•é¡Œã®ã¿å…¬é–‹**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›ã™ã‚‹å•é¡Œã¯100% AIç”Ÿæˆ
- âœ… **é¡ä¼¼åº¦ç›£è¦–**: Embedding-basedæ¤œå‡ºã§è‘—ä½œæ¨©ä¾µå®³ã‚’é˜²æ­¢
- âœ… **ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³**: ãƒãƒƒãƒå‡¦ç†ã§åŸå­æ€§ä¿è¨¼
- âœ… **å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„**: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ä¿è¨¼

---

## ğŸ”’ è‘—ä½œæ¨©å®‰å…¨è¨­è¨ˆï¼ˆæœ€é‡è¦ï¼‰

### å•é¡Œ: V1ã®è‘—ä½œæ¨©ãƒªã‚¹ã‚¯

```sql
-- âŒ V1ã®å±é™ºãªè¨­è¨ˆï¼ˆå‰Šé™¤ï¼‰
CREATE TABLE questions (
    question_text TEXT NOT NULL,      -- ğŸš¨ è‘—ä½œæ¨©é•åãƒªã‚¹ã‚¯
    choices_json TEXT NOT NULL,       -- ğŸš¨ è‘—ä½œæ¨©é•åãƒªã‚¹ã‚¯
    correct_answer_index INTEGER,
    explanation TEXT,
    ...
);
```

### è§£æ±ºç­–: åˆ†æçµæœã®ã¿ä¿å­˜

```sql
-- âœ… V2ã®å®‰å…¨è¨­è¨ˆ
CREATE TABLE IF NOT EXISTS question_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼ˆè‘—ä½œæ¨©å¯¾è±¡å¤–ï¼‰
    grade TEXT NOT NULL,                    -- '5','4','3','pre2','2','pre1','1'
    section TEXT NOT NULL,                  -- 'reading_1', 'listening_2'
    question_number INTEGER,                -- ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®é€šã—ç•ªå·
    question_type TEXT NOT NULL,            -- 'vocabulary', 'grammar', 'reading_comp'
    
    -- åˆ†æçµæœï¼ˆè‘—ä½œæ¨©å¯¾è±¡å¤–ï¼‰
    grammar_patterns TEXT NOT NULL,         -- JSON: ['present_perfect', 'passive_voice']
    vocabulary_level TEXT NOT NULL,         -- 'CEFR-B1', 'CEFR-B2'
    sentence_structure TEXT NOT NULL,       -- 'complex', 'compound'
    difficulty_score REAL NOT NULL,         -- 0.0-1.0ï¼ˆçµ±è¨ˆãƒ‡ãƒ¼ã‚¿ï¼‰
    
    -- èª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆè‘—ä½œæ¨©å¯¾è±¡å¤–ï¼‰
    distractor_patterns TEXT NOT NULL,      -- JSON: {'type': 'tense_confusion', 'level': 'common'}
    common_errors TEXT,                     -- JSON: ã‚ˆãã‚ã‚‹é–“é•ã„ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    
    -- ãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£ï¼ˆå†…éƒ¨ç®¡ç†ç”¨ï¼‰
    source_year INTEGER,                    -- å®Ÿæ–½å¹´
    source_session TEXT,                    -- '1st', '2nd', '3rd'
    analysis_date TEXT DEFAULT CURRENT_TIMESTAMP,
    
    -- Embeddingï¼ˆé¡ä¼¼åº¦æ¤œå‡ºç”¨ï¼‰
    pattern_embedding_hash TEXT,            -- ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒã‚·ãƒ¥ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
    
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    
    CHECK (difficulty_score >= 0.0 AND difficulty_score <= 1.0)
);

-- ğŸ”´ é‡è¦: å®Ÿéš›ã®å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã¯ä¿å­˜ã—ãªã„
-- éå»å•PDFã¯å¤–éƒ¨ã®å®‰å…¨ãªå ´æ‰€ã«ä¿ç®¡ï¼ˆDBå¤–ï¼‰
-- åˆ†ææ™‚ã®ã¿èª­ã¿è¾¼ã¿ã€çµæœã®ã¿DBã«ä¿å­˜
```

### è‘—ä½œæ¨©å®‰å…¨ãªé‹ç”¨ãƒ•ãƒ­ãƒ¼

```typescript
// âŒ V1ã®å±é™ºãªå‡¦ç†ï¼ˆå‰Šé™¤ï¼‰
async function storeOriginalQuestion(question: OriginalQuestion) {
  // ğŸš¨ ã“ã‚Œã¯è‘—ä½œæ¨©é•åãƒªã‚¹ã‚¯
  await env.DB.prepare(`
    INSERT INTO questions (question_text, choices_json, ...)
    VALUES (?, ?, ...)
  `).bind(question.text, JSON.stringify(question.choices)).run();
}

// âœ… V2ã®å®‰å…¨ãªå‡¦ç†
interface QuestionAnalysisResult {
  grammar_patterns: string[];
  vocabulary_level: string;
  sentence_structure: string;
  difficulty_score: number;
  distractor_patterns: {
    type: string;
    level: string;
  };
}

async function analyzeAndStore(
  originalQuestion: OriginalQuestion, // ãƒ¡ãƒ¢ãƒªå†…ã®ã¿ï¼ˆDBã«ã¯ä¿å­˜ã—ãªã„ï¼‰
  env: Env
): Promise<void> {
  // 1. AIåˆ†æï¼ˆå•é¡Œã®ç‰¹å¾´ã‚’æŠ½å‡ºï¼‰
  const analysis = await analyzeQuestion(originalQuestion);
  
  // 2. åˆ†æçµæœã®ã¿DBã«ä¿å­˜
  await env.DB.prepare(`
    INSERT INTO question_analysis (
      grade, section, question_type,
      grammar_patterns, vocabulary_level, sentence_structure,
      difficulty_score, distractor_patterns,
      source_year, source_session
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
  `).bind(
    originalQuestion.grade,
    originalQuestion.section,
    originalQuestion.type,
    JSON.stringify(analysis.grammar_patterns),
    analysis.vocabulary_level,
    analysis.sentence_structure,
    analysis.difficulty_score,
    JSON.stringify(analysis.distractor_patterns),
    originalQuestion.year,
    originalQuestion.session
  ).run();
  
  // 3. å…ƒã®å•é¡Œæ–‡ã¯ç ´æ£„ï¼ˆãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤ï¼‰
  // ã“ã‚Œã«ã‚ˆã‚Šè‘—ä½œæ¨©ä¾µå®³ã‚’é˜²ã
}

// AIåˆ†æé–¢æ•°ï¼ˆå•é¡Œã®ç‰¹å¾´ã‚’æŠ½å‡ºï¼‰
async function analyzeQuestion(
  question: OriginalQuestion
): Promise<QuestionAnalysisResult> {
  const prompt = `
ã‚ãªãŸã¯è‹±èªå•é¡Œã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å•é¡Œã‚’åˆ†æã—ã€å•é¡Œã®**ç‰¹å¾´ã®ã¿**ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
**å•é¡Œæ–‡ã‚„é¸æŠè‚¢ã‚’å‡ºåŠ›ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**

å•é¡Œæ–‡: ${question.text}
é¸æŠè‚¢: ${JSON.stringify(question.choices)}
æ­£è§£: ${question.correct_answer}

ä»¥ä¸‹ã®æƒ…å ±ã®ã¿æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: present_perfect, passive_voiceï¼‰
2. èªå½™ãƒ¬ãƒ™ãƒ«ï¼ˆCEFRåŸºæº–ï¼‰
3. æ–‡ã®æ§‹é€ ï¼ˆsimple, compound, complexï¼‰
4. é›£æ˜“åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
5. èª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¨®é¡ï¼ˆä¾‹: tense_confusion, word_form_errorï¼‰

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{
  "grammar_patterns": ["pattern1", "pattern2"],
  "vocabulary_level": "CEFR-B1",
  "sentence_structure": "complex",
  "difficulty_score": 0.65,
  "distractor_patterns": {
    "type": "tense_confusion",
    "level": "common"
  }
}
`;

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Extract question features only. Do not include original text.' },
        { role: 'user', content: prompt }
      ],
      temperature: 0.1, // ä½æ¸©åº¦ã§ä¸€è²«æ€§ã‚’ä¿ã¤
      response_format: { type: 'json_object' }
    }),
  });

  const data = await response.json();
  return JSON.parse(data.choices[0].message.content);
}
```

---

## ğŸ—„ï¸ å®Œå…¨ãªã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼ˆV2ï¼‰

```sql
-- ====================
-- åˆæœŸåŒ–: å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’æœ‰åŠ¹åŒ–
-- ====================
PRAGMA foreign_keys = ON;

-- ====================
-- 1. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»èªè¨¼
-- ====================

-- å­¦ç”Ÿãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
CREATE TABLE IF NOT EXISTS student_profiles (
    id TEXT PRIMARY KEY,                    -- UUID ã¾ãŸã¯ Auth0 ID
    email TEXT UNIQUE NOT NULL,
    display_name TEXT,
    target_grade TEXT NOT NULL,             -- ç›®æ¨™ç´š
    registration_date TEXT DEFAULT CURRENT_TIMESTAMP,
    last_login TEXT,
    account_status TEXT DEFAULT 'active',   -- 'active', 'suspended', 'deleted'
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (account_status IN ('active', 'suspended', 'deleted'))
);

-- ç›£æŸ»ãƒ­ã‚°ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ï¼‰
CREATE TABLE IF NOT EXISTS audit_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    student_id TEXT NOT NULL,
    action_type TEXT NOT NULL,              -- 'login', 'question_solved', 'data_export'
    resource_type TEXT,                     -- 'question', 'session', 'profile'
    resource_id TEXT,
    ip_address TEXT,
    user_agent TEXT,
    timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT,                          -- JSON: è¿½åŠ æƒ…å ±
    FOREIGN KEY (student_id) REFERENCES student_profiles(id) ON DELETE CASCADE,
    CHECK (json_valid(metadata) OR metadata IS NULL)
);

-- ====================
-- 2. å•é¡Œåˆ†æï¼ˆè‘—ä½œæ¨©å®‰å…¨ç‰ˆï¼‰
-- ====================

-- éå»å•åˆ†æçµæœï¼ˆå•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã¯ä¿å­˜ã—ãªã„ï¼‰
CREATE TABLE IF NOT EXISTS question_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    grade TEXT NOT NULL,
    section TEXT NOT NULL,
    question_number INTEGER,
    question_type TEXT NOT NULL,
    
    -- åˆ†æçµæœã®ã¿
    grammar_patterns TEXT NOT NULL,         -- JSON array
    vocabulary_level TEXT NOT NULL,
    sentence_structure TEXT NOT NULL,
    difficulty_score REAL NOT NULL,
    distractor_patterns TEXT NOT NULL,      -- JSON object
    common_errors TEXT,                     -- JSON array
    
    source_year INTEGER,
    source_session TEXT,
    analysis_date TEXT DEFAULT CURRENT_TIMESTAMP,
    pattern_embedding_hash TEXT,            -- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨
    
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    
    CHECK (difficulty_score >= 0.0 AND difficulty_score <= 1.0),
    CHECK (json_valid(grammar_patterns)),
    CHECK (json_valid(distractor_patterns)),
    CHECK (json_valid(common_errors) OR common_errors IS NULL)
);

-- ä¸€æ„åˆ¶ç´„: åŒã˜å•é¡Œã‚’é‡è¤‡åˆ†æã—ãªã„
CREATE UNIQUE INDEX IF NOT EXISTS uq_analysis_place
    ON question_analysis(grade, section, question_number, source_year, source_session);

-- ====================
-- 3. AIç”Ÿæˆå•é¡Œï¼ˆå…¬é–‹ç”¨ï¼‰
-- ====================

-- AIç”Ÿæˆå•é¡Œï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›ã™ã‚‹å”¯ä¸€ã®å•é¡Œï¼‰
CREATE TABLE IF NOT EXISTS generated_questions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    analysis_id INTEGER,                    -- åŸºã¨ãªã£ãŸåˆ†æIDï¼ˆnullableï¼‰
    grade TEXT NOT NULL,
    section TEXT NOT NULL,
    question_type TEXT NOT NULL,
    
    -- å•é¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆAIç”Ÿæˆã®ã¿ï¼‰
    question_text TEXT NOT NULL,
    choices_json TEXT NOT NULL,
    correct_answer_index INTEGER NOT NULL,
    explanation TEXT,
    explanation_ja TEXT,
    audio_key TEXT,                         -- R2ã®ã‚­ãƒ¼ï¼ˆTTSç”Ÿæˆï¼‰
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    difficulty_score REAL DEFAULT 0.5,
    vocab_band TEXT,
    
    -- AIç”Ÿæˆæƒ…å ±
    model TEXT NOT NULL,                    -- 'gpt-4o', 'gpt-4-turbo'
    temperature REAL,
    prompt_hash TEXT,
    seed INTEGER,
    generation_timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
    
    -- å“è³ªç®¡ç†
    similarity_score REAL,                  -- æ—¢å­˜å•é¡Œã¨ã®é¡ä¼¼åº¦
    review_status TEXT DEFAULT 'pending',   -- 'pending', 'approved', 'rejected'
    reviewed_by TEXT,
    reviewed_at TEXT,
    quality_score REAL,                     -- 1-5
    
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    
    CHECK (json_valid(choices_json)),
    CHECK (correct_answer_index >= 0 AND correct_answer_index < 4),
    CHECK (review_status IN ('pending', 'approved', 'rejected')),
    CHECK (difficulty_score >= 0.0 AND difficulty_score <= 1.0),
    CHECK (quality_score IS NULL OR (quality_score >= 1.0 AND quality_score <= 5.0)),
    FOREIGN KEY (analysis_id) REFERENCES question_analysis(id) ON DELETE SET NULL
);

-- ä¸€æ„åˆ¶ç´„: åŒä¸€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã§å•é¡Œç•ªå·ã‚’é‡è¤‡ã•ã›ãªã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
-- CREATE UNIQUE INDEX IF NOT EXISTS uq_generated_place
--     ON generated_questions(grade, section, id);

-- ====================
-- 4. ã‚¿ã‚°ç®¡ç†
-- ====================

CREATE TABLE IF NOT EXISTS tags (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    type TEXT NOT NULL,                     -- 'grammar', 'vocabulary', 'topic'
    category TEXT,
    description TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (type IN ('grammar', 'vocabulary', 'topic'))
);

-- ç”Ÿæˆå•é¡Œã¨ã‚¿ã‚°ã®é–¢é€£ï¼ˆä¸­é–“ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
CREATE TABLE IF NOT EXISTS generated_question_tags (
    question_id INTEGER NOT NULL,
    tag_id INTEGER NOT NULL,
    relevance_score REAL DEFAULT 1.0,
    PRIMARY KEY (question_id, tag_id),
    CHECK (relevance_score >= 0.0 AND relevance_score <= 1.0),
    FOREIGN KEY (question_id) REFERENCES generated_questions(id) ON DELETE CASCADE,
    FOREIGN KEY (tag_id) REFERENCES tags(id) ON DELETE CASCADE
);

-- ====================
-- 5. å­¦ç¿’ç®¡ç†
-- ====================

CREATE TABLE IF NOT EXISTS learning_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    student_id TEXT NOT NULL,
    session_type TEXT NOT NULL,
    grade TEXT NOT NULL,
    section TEXT,
    started_at TEXT DEFAULT CURRENT_TIMESTAMP,
    completed_at TEXT,
    total_questions INTEGER DEFAULT 0,
    correct_count INTEGER DEFAULT 0,
    time_limit_seconds INTEGER,
    metadata TEXT,
    CHECK (session_type IN ('practice', 'mock_test', 'review', 'weak_point')),
    CHECK (json_valid(metadata) OR metadata IS NULL),
    FOREIGN KEY (student_id) REFERENCES student_profiles(id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS learning_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id INTEGER NOT NULL,
    student_id TEXT NOT NULL,
    question_id INTEGER NOT NULL,
    grade TEXT NOT NULL,
    section TEXT NOT NULL,
    question_type TEXT NOT NULL,
    user_answer_index INTEGER,
    correct_answer_index INTEGER NOT NULL,
    is_correct INTEGER NOT NULL,
    time_spent_ms INTEGER,
    confidence_level INTEGER,
    device TEXT,
    started_at TEXT,
    answered_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (is_correct IN (0, 1)),
    CHECK (user_answer_index IS NULL OR (user_answer_index >= 0 AND user_answer_index < 4)),
    CHECK (confidence_level IS NULL OR (confidence_level >= 1 AND confidence_level <= 5)),
    FOREIGN KEY (session_id) REFERENCES learning_sessions(id) ON DELETE CASCADE,
    FOREIGN KEY (student_id) REFERENCES student_profiles(id) ON DELETE CASCADE,
    FOREIGN KEY (question_id) REFERENCES generated_questions(id) ON DELETE CASCADE
);

-- å­¦ç”Ÿçµ±è¨ˆã‚µãƒãƒªãƒ¼
CREATE TABLE IF NOT EXISTS student_stats (
    student_id TEXT PRIMARY KEY,
    grade TEXT NOT NULL,
    total_questions INTEGER DEFAULT 0,
    correct_answers INTEGER DEFAULT 0,
    accuracy_rate REAL DEFAULT 0.0,
    total_study_time_ms INTEGER DEFAULT 0,
    study_days INTEGER DEFAULT 0,
    current_streak INTEGER DEFAULT 0,
    last_study_date TEXT,
    weak_tags TEXT,                         -- JSON array
    strong_tags TEXT,                       -- JSON array
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (accuracy_rate >= 0.0 AND accuracy_rate <= 1.0),
    CHECK (json_valid(weak_tags) OR weak_tags IS NULL),
    CHECK (json_valid(strong_tags) OR strong_tags IS NULL),
    FOREIGN KEY (student_id) REFERENCES student_profiles(id) ON DELETE CASCADE
);

-- å¾©ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆSRSï¼‰
CREATE TABLE IF NOT EXISTS review_schedule (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    student_id TEXT NOT NULL,
    question_id INTEGER NOT NULL,
    ease_factor REAL DEFAULT 2.5,
    interval_days INTEGER DEFAULT 1,
    repetitions INTEGER DEFAULT 0,
    next_review_date TEXT NOT NULL,
    last_reviewed_at TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (ease_factor >= 1.3),
    CHECK (interval_days >= 0),
    CHECK (repetitions >= 0),
    FOREIGN KEY (student_id) REFERENCES student_profiles(id) ON DELETE CASCADE,
    FOREIGN KEY (question_id) REFERENCES generated_questions(id) ON DELETE CASCADE
);

-- ====================
-- 6. ãƒ¡ãƒ‡ã‚£ã‚¢ç®¡ç†
-- ====================

CREATE TABLE IF NOT EXISTS media_assets (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    r2_key TEXT NOT NULL UNIQUE,
    asset_type TEXT NOT NULL,
    mime_type TEXT NOT NULL,
    file_size_bytes INTEGER,
    duration_seconds REAL,
    width INTEGER,
    height INTEGER,
    source TEXT,                            -- 'openai_tts', 'elevenlabs', 'upload'
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (asset_type IN ('audio', 'image'))
);

-- ====================
-- 7. AIå“è³ªç®¡ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥
-- ====================

-- Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
CREATE TABLE IF NOT EXISTS embedding_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    text_hash TEXT NOT NULL UNIQUE,         -- SHA-256 hash
    model TEXT NOT NULL,                    -- 'text-embedding-3-small'
    embedding_json TEXT NOT NULL,           -- JSON array of floats
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    last_used_at TEXT DEFAULT CURRENT_TIMESTAMP,
    use_count INTEGER DEFAULT 1,
    CHECK (json_valid(embedding_json))
);

-- AIç”Ÿæˆãƒ­ã‚°
CREATE TABLE IF NOT EXISTS generation_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    request_id TEXT NOT NULL UNIQUE,
    analysis_id INTEGER,
    model TEXT NOT NULL,
    temperature REAL,
    prompt_text TEXT,
    response_text TEXT,
    generation_time_ms INTEGER,
    tokens_used INTEGER,
    success INTEGER NOT NULL,
    error_message TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (success IN (0, 1)),
    FOREIGN KEY (analysis_id) REFERENCES question_analysis(id) ON DELETE SET NULL
);

-- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
CREATE TABLE IF NOT EXISTS question_feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    question_id INTEGER NOT NULL,
    feedback_type TEXT NOT NULL,
    rating INTEGER,
    comment TEXT,
    submitted_by TEXT NOT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    CHECK (feedback_type IN ('quality', 'difficulty', 'error', 'clarity')),
    CHECK (rating IS NULL OR (rating >= 1 AND rating <= 5)),
    FOREIGN KEY (question_id) REFERENCES generated_questions(id) ON DELETE CASCADE,
    FOREIGN KEY (submitted_by) REFERENCES student_profiles(id) ON DELETE CASCADE
);

-- ====================
-- 8. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
-- ====================

-- å•é¡Œåˆ†æç”¨
CREATE INDEX IF NOT EXISTS idx_analysis_grade_section 
    ON question_analysis(grade, section);
CREATE INDEX IF NOT EXISTS idx_analysis_type 
    ON question_analysis(question_type);

-- ç”Ÿæˆå•é¡Œç”¨
CREATE INDEX IF NOT EXISTS idx_gen_questions_grade_section 
    ON generated_questions(grade, section);
CREATE INDEX IF NOT EXISTS idx_gen_questions_status 
    ON generated_questions(review_status);
CREATE INDEX IF NOT EXISTS idx_gen_questions_analysis 
    ON generated_questions(analysis_id);

-- ã‚¿ã‚°ç”¨
CREATE INDEX IF NOT EXISTS idx_gen_question_tags_tag 
    ON generated_question_tags(tag_id);

-- å­¦ç¿’å±¥æ­´ç”¨
CREATE INDEX IF NOT EXISTS idx_history_student_time 
    ON learning_history(student_id, answered_at DESC);
CREATE INDEX IF NOT EXISTS idx_history_student_correct 
    ON learning_history(student_id, is_correct);
CREATE INDEX IF NOT EXISTS idx_history_session 
    ON learning_history(session_id);

-- å¾©ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”¨
CREATE INDEX IF NOT EXISTS idx_review_student_date 
    ON review_schedule(student_id, next_review_date);

-- Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨
CREATE INDEX IF NOT EXISTS idx_embedding_cache_hash 
    ON embedding_cache(text_hash);
CREATE INDEX IF NOT EXISTS idx_embedding_cache_last_used 
    ON embedding_cache(last_used_at);

-- ç›£æŸ»ãƒ­ã‚°ç”¨
CREATE INDEX IF NOT EXISTS idx_audit_student_time 
    ON audit_logs(student_id, timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_audit_action 
    ON audit_logs(action_type);

-- ====================
-- 9. è‡ªå‹•æ›´æ–°ãƒˆãƒªã‚¬ãƒ¼
-- ====================

-- student_profiles ã® updated_at è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_student_profiles_updated_at
AFTER UPDATE ON student_profiles
FOR EACH ROW
BEGIN
    UPDATE student_profiles 
    SET updated_at = CURRENT_TIMESTAMP 
    WHERE id = OLD.id;
END;

-- question_analysis ã® updated_at è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_question_analysis_updated_at
AFTER UPDATE ON question_analysis
FOR EACH ROW
BEGIN
    UPDATE question_analysis 
    SET updated_at = CURRENT_TIMESTAMP 
    WHERE id = OLD.id;
END;

-- generated_questions ã® updated_at è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_generated_questions_updated_at
AFTER UPDATE ON generated_questions
FOR EACH ROW
BEGIN
    UPDATE generated_questions 
    SET updated_at = CURRENT_TIMESTAMP 
    WHERE id = OLD.id;
END;

-- student_stats ã® updated_at è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_student_stats_updated_at
AFTER UPDATE ON student_stats
FOR EACH ROW
BEGIN
    UPDATE student_stats 
    SET updated_at = CURRENT_TIMESTAMP 
    WHERE student_id = OLD.student_id;
END;

-- review_schedule ã® updated_at è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_review_schedule_updated_at
AFTER UPDATE ON review_schedule
FOR EACH ROW
BEGIN
    UPDATE review_schedule 
    SET updated_at = CURRENT_TIMESTAMP 
    WHERE id = OLD.id;
END;

-- Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ã® last_used_at ã¨ use_count è‡ªå‹•æ›´æ–°
CREATE TRIGGER IF NOT EXISTS trg_embedding_cache_used
AFTER UPDATE ON embedding_cache
FOR EACH ROW
WHEN NEW.last_used_at = OLD.last_used_at
BEGIN
    UPDATE embedding_cache 
    SET last_used_at = CURRENT_TIMESTAMP,
        use_count = use_count + 1
    WHERE id = OLD.id;
END;

-- ====================
-- 10. åˆ†æç”¨ãƒ“ãƒ¥ãƒ¼
-- ====================

-- å­¦ç”Ÿã®å¼±ç‚¹ã‚¿ã‚°
CREATE VIEW IF NOT EXISTS student_weak_points AS
SELECT 
    lh.student_id,
    t.id AS tag_id,
    t.name AS tag_name,
    t.type AS tag_type,
    COUNT(*) AS total_attempts,
    SUM(lh.is_correct) AS correct_count,
    CAST(SUM(lh.is_correct) AS REAL) / COUNT(*) AS accuracy,
    AVG(lh.time_spent_ms) AS avg_time_ms
FROM learning_history lh
INNER JOIN generated_question_tags gqt ON gqt.question_id = lh.question_id
INNER JOIN tags t ON t.id = gqt.tag_id
GROUP BY lh.student_id, t.id
HAVING accuracy < 0.7 AND total_attempts >= 3;

-- ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
CREATE VIEW IF NOT EXISTS student_section_performance AS
SELECT 
    student_id,
    grade,
    section,
    COUNT(*) AS total_questions,
    SUM(is_correct) AS correct_answers,
    CAST(SUM(is_correct) AS REAL) / COUNT(*) AS accuracy,
    AVG(time_spent_ms) AS avg_time_ms
FROM learning_history
GROUP BY student_id, grade, section;

-- å•é¡Œé›£æ˜“åº¦çµ±è¨ˆ
CREATE VIEW IF NOT EXISTS question_difficulty_stats AS
SELECT 
    gq.id AS question_id,
    gq.grade,
    gq.section,
    gq.question_type,
    gq.difficulty_score AS expected_difficulty,
    COUNT(lh.id) AS attempts,
    CAST(SUM(lh.is_correct) AS REAL) / COUNT(lh.id) AS actual_accuracy,
    AVG(lh.time_spent_ms) AS avg_time_ms
FROM generated_questions gq
LEFT JOIN learning_history lh ON lh.question_id = gq.id
GROUP BY gq.id
HAVING attempts >= 10;
```

---

## ğŸ¤– AIå•é¡Œç”Ÿæˆï¼ˆè‘—ä½œæ¨©å®‰å…¨ç‰ˆï¼‰

### Phase 1: éå»å•åˆ†æï¼ˆå†…éƒ¨å‡¦ç†ã®ã¿ï¼‰

```typescript
// éå»å•PDFã¯å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ï¼ˆä¾‹: æš—å·åŒ–ã•ã‚ŒãŸç®¡ç†è€…å°‚ç”¨R2ãƒã‚±ãƒƒãƒˆï¼‰
// DBã«ã¯åˆ†æçµæœã®ã¿ä¿å­˜

interface OriginalQuestion {
  text: string;
  choices: string[];
  correct_answer: number;
  grade: string;
  section: string;
  year: number;
  session: string;
}

// éå»å•ã‚’åˆ†æã—ã€ç‰¹å¾´ã‚’æŠ½å‡ºï¼ˆå•é¡Œæ–‡ã¯ä¿å­˜ã—ãªã„ï¼‰
async function analyzeOriginalQuestion(
  question: OriginalQuestion,
  env: Env
): Promise<number> {
  const analysisPrompt = `
ã‚ãªãŸã¯è‹±èªå•é¡Œã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å•é¡Œã‚’åˆ†æã—ã€**å•é¡Œã®ç‰¹å¾´ã®ã¿**ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
**å…ƒã®å•é¡Œæ–‡ã‚„é¸æŠè‚¢ã‚’å‡ºåŠ›ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**

å•é¡Œæ–‡: ${question.text}
é¸æŠè‚¢: ${JSON.stringify(question.choices)}
æ­£è§£ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: ${question.correct_answer}

æŠ½å‡ºã™ã‚‹æƒ…å ±:
1. æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé…åˆ—ï¼‰
2. èªå½™ãƒ¬ãƒ™ãƒ«ï¼ˆCEFRåŸºæº–ï¼‰
3. æ–‡ã®æ§‹é€ ï¼ˆsimple/compound/complexï¼‰
4. é›£æ˜“åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
5. èª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¨®é¡ã¨ç‰¹å¾´

å‡ºåŠ›å½¢å¼:
{
  "grammar_patterns": ["pattern1", "pattern2"],
  "vocabulary_level": "CEFR-B1",
  "sentence_structure": "complex",
  "difficulty_score": 0.65,
  "distractor_patterns": {
    "type": "tense_confusion",
    "level": "common",
    "characteristics": ["similar_forms", "context_dependent"]
  },
  "common_errors": ["mistake_type_1", "mistake_type_2"]
}
`;

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Extract question features only. Never include original text.' },
        { role: 'user', content: analysisPrompt }
      ],
      temperature: 0.1,
      response_format: { type: 'json_object' }
    }),
  });

  const data = await response.json();
  const analysis = JSON.parse(data.choices[0].message.content);

  // åˆ†æçµæœã®ã¿ã‚’DBã«ä¿å­˜ï¼ˆåŸå­æ€§ä¿è¨¼ï¼‰
  const result = await env.DB.prepare(`
    INSERT INTO question_analysis (
      grade, section, question_number, question_type,
      grammar_patterns, vocabulary_level, sentence_structure,
      difficulty_score, distractor_patterns, common_errors,
      source_year, source_session
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
  `).bind(
    question.grade,
    question.section,
    extractQuestionNumber(question.text), // ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    classifyQuestionType(question.text),  // ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    JSON.stringify(analysis.grammar_patterns),
    analysis.vocabulary_level,
    analysis.sentence_structure,
    analysis.difficulty_score,
    JSON.stringify(analysis.distractor_patterns),
    JSON.stringify(analysis.common_errors),
    question.year,
    question.session
  ).run();

  return result.meta.last_row_id as number;
}
```

### Phase 2: AIå•é¡Œç”Ÿæˆï¼ˆåˆ†æã«åŸºã¥ãï¼‰

```typescript
interface GeneratedQuestionDraft {
  question_text: string;
  choices: string[];
  correct_answer_index: number;
  explanation: string;
  explanation_ja: string;
  distractor_rationale: Record<number, string>;
}

// åˆ†æçµæœã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«å•é¡Œã‚’ç”Ÿæˆ
async function generateQuestionFromAnalysis(
  analysisId: number,
  env: Env
): Promise<GeneratedQuestionDraft> {
  // 1. åˆ†æçµæœã‚’å–å¾—
  const analysis = await env.DB.prepare(`
    SELECT * FROM question_analysis WHERE id = ?
  `).bind(analysisId).first();

  if (!analysis) {
    throw new Error(`Analysis not found: ${analysisId}`);
  }

  const grammarPatterns = JSON.parse(analysis.grammar_patterns as string);
  const distractorPatterns = JSON.parse(analysis.distractor_patterns as string);

  // 2. ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆéå»å•ã®å†…å®¹ã¯å«ã¾ãªã„ï¼‰
  const generationPrompt = `
ã‚ãªãŸã¯è‹±æ¤œ${analysis.grade}ç´šã®å•é¡Œä½œæˆå°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®**ç‰¹å¾´ã‚’æŒã¤**å®Œå…¨ã«ã‚ªãƒªã‚¸ãƒŠãƒ«ãªå•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

# å•é¡Œã®ç‰¹å¾´ï¼ˆå‚è€ƒæƒ…å ±ï¼‰
- ç´š: ${analysis.grade}
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³: ${analysis.section}
- å•é¡Œã‚¿ã‚¤ãƒ—: ${analysis.question_type}
- æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³: ${grammarPatterns.join(', ')}
- èªå½™ãƒ¬ãƒ™ãƒ«: ${analysis.vocabulary_level}
- æ–‡ã®æ§‹é€ : ${analysis.sentence_structure}
- é›£æ˜“åº¦ã‚¹ã‚³ã‚¢: ${analysis.difficulty_score}
- èª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³: ${distractorPatterns.type}

# å¿…é ˆè¦ä»¶
1. **å®Œå…¨ã«ã‚ªãƒªã‚¸ãƒŠãƒ«**ãªæ–‡è„ˆãƒ»ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
2. ä¸Šè¨˜ã®æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹å•é¡Œ
3. æŒ‡å®šã•ã‚ŒãŸèªå½™ãƒ¬ãƒ™ãƒ«ï¼ˆ${analysis.vocabulary_level}ï¼‰ã‚’ç¶­æŒ
4. èª¤ç­”é¸æŠè‚¢ã¯ã€Œ${distractorPatterns.type}ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†
5. é›£æ˜“åº¦ã‚¹ã‚³ã‚¢ ${analysis.difficulty_score} ã«ç›¸å½“

# ç¦æ­¢äº‹é …
- æ—¢å­˜ã®å•é¡Œã®å˜èªå…¥ã‚Œæ›¿ãˆã®ã¿
- å›ºæœ‰åè©ã®æµç”¨
- æ•°å­—ã‚„æ—¥ä»˜ã®å˜ç´”å¤‰æ›´

# å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{
  "question_text": "å®Œå…¨ã«ã‚ªãƒªã‚¸ãƒŠãƒ«ãªå•é¡Œæ–‡",
  "choices": ["é¸æŠè‚¢A", "é¸æŠè‚¢B", "é¸æŠè‚¢C", "é¸æŠè‚¢D"],
  "correct_answer_index": 0,
  "explanation": "æ­£è§£ã®ç†ç”±ï¼ˆè‹±èªï¼‰",
  "explanation_ja": "æ­£è§£ã®ç†ç”±ï¼ˆæ—¥æœ¬èªï¼‰",
  "distractor_rationale": {
    "1": "é¸æŠè‚¢BãŒèª¤ç­”ã®ç†ç”±",
    "2": "é¸æŠè‚¢CãŒèª¤ç­”ã®ç†ç”±",
    "3": "é¸æŠè‚¢DãŒèª¤ç­”ã®ç†ç”±"
  }
}
`;

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'You are an EIKEN question creator. Generate completely original questions.' },
        { role: 'user', content: generationPrompt }
      ],
      temperature: 0.7, // å‰µé€ æ€§ã‚’é«˜ã‚ã‚‹
      response_format: { type: 'json_object' }
    }),
  });

  const data = await response.json();
  return JSON.parse(data.choices[0].message.content);
}
```

### Phase 3: å“è³ªæ¤œè¨¼ã¨DBä¿å­˜ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

```typescript
interface ValidationResult {
  isValid: boolean;
  score: number;
  issues: string[];
  warnings: string[];
  similarity_score: number;
}

async function validateAndSave(
  draft: GeneratedQuestionDraft,
  analysisId: number,
  env: Env
): Promise<number> {
  // 1. åŸºæœ¬æ¤œè¨¼
  const validation = await validateGeneratedQuestion(draft, analysisId, env);

  if (!validation.isValid) {
    throw new Error(`Validation failed: ${validation.issues.join(', ')}`);
  }

  // 2. é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆEmbeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
  const similarityScore = await checkSimilarityWithCache(
    draft.question_text,
    analysisId,
    env
  );

  if (similarityScore > 0.85) {
    throw new Error(`Similarity too high: ${similarityScore.toFixed(2)}`);
  }

  // 3. ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä¿å­˜ï¼ˆD1 batchï¼‰
  const analysis = await env.DB.prepare(`
    SELECT grade, section, question_type FROM question_analysis WHERE id = ?
  `).bind(analysisId).first();

  const promptHash = await generatePromptHash(analysisId);

  // âœ… D1ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåŸå­æ€§ä¿è¨¼ï¼‰
  const statements = [
    // 3-1. ç”Ÿæˆå•é¡Œã‚’ä¿å­˜
    env.DB.prepare(`
      INSERT INTO generated_questions (
        analysis_id, grade, section, question_type,
        question_text, choices_json, correct_answer_index,
        explanation, explanation_ja,
        model, temperature, prompt_hash, similarity_score,
        review_status
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `).bind(
      analysisId,
      analysis.grade,
      analysis.section,
      analysis.question_type,
      draft.question_text,
      JSON.stringify(draft.choices),
      draft.correct_answer_index,
      draft.explanation,
      draft.explanation_ja,
      'gpt-4o',
      0.7,
      promptHash,
      similarityScore,
      validation.score >= 80 ? 'approved' : 'pending'
    ),
  ];

  // 3-2. ã‚¿ã‚°ã‚’é–¢é€£ä»˜ã‘ï¼ˆæ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ï¼‰
  const grammarPatterns = JSON.parse(
    (await env.DB.prepare(`SELECT grammar_patterns FROM question_analysis WHERE id = ?`)
      .bind(analysisId).first())?.grammar_patterns as string || '[]'
  );

  for (const pattern of grammarPatterns) {
    // ã‚¿ã‚°IDã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
    let tag = await env.DB.prepare(`SELECT id FROM tags WHERE name = ?`)
      .bind(pattern).first();

    let tagId: number;
    if (!tag) {
      const tagResult = await env.DB.prepare(`
        INSERT INTO tags (name, type) VALUES (?, 'grammar')
      `).bind(pattern).run();
      tagId = tagResult.meta.last_row_id as number;
    } else {
      tagId = tag.id as number;
    }

    // ä¸­é–“ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²ï¼ˆlast_row_idã‚’ä½¿ç”¨ï¼‰
    statements.push(
      env.DB.prepare(`
        INSERT INTO generated_question_tags (question_id, tag_id)
        SELECT last_insert_rowid(), ?
      `).bind(tagId)
    );
  }

  // âœ… ãƒãƒƒãƒå®Ÿè¡Œï¼ˆå…¨ã¦æˆåŠŸã™ã‚‹ã‹ã€å…¨ã¦å¤±æ•—ï¼‰
  const results = await env.DB.batch(statements);

  // æœ€åˆã®INSERTã®çµæœã‹ã‚‰IDã‚’å–å¾—
  const questionId = results[0].meta.last_row_id as number;

  console.log(`âœ… Generated question saved: ID ${questionId}, Similarity: ${similarityScore.toFixed(2)}`);

  return questionId;
}

// æ¤œè¨¼é–¢æ•°
async function validateGeneratedQuestion(
  draft: GeneratedQuestionDraft,
  analysisId: number,
  env: Env
): Promise<ValidationResult> {
  const issues: string[] = [];
  const warnings: string[] = [];
  let score = 100;

  // åŸºæœ¬ãƒã‚§ãƒƒã‚¯
  if (!Array.isArray(draft.choices) || draft.choices.length !== 4) {
    issues.push('é¸æŠè‚¢ãŒ4ã¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“');
    score -= 50;
  }

  if (draft.correct_answer_index < 0 || draft.correct_answer_index > 3) {
    issues.push('æ­£è§£ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸æ­£ã§ã™');
    score -= 50;
  }

  // é¸æŠè‚¢ã®ä¸€æ„æ€§
  const uniqueChoices = new Set(draft.choices.map(c => c.trim().toLowerCase()));
  if (uniqueChoices.size !== 4) {
    issues.push('é¸æŠè‚¢ã«é‡è¤‡ãŒã‚ã‚Šã¾ã™');
    score -= 30;
  }

  // ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³
  const prohibitedPatterns = [/\[.*?\]/, /___+/, /TODO/i, /FIXME/i];
  for (const pattern of prohibitedPatterns) {
    if (pattern.test(draft.question_text)) {
      issues.push('ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ');
      score -= 30;
    }
  }

  return {
    isValid: issues.length === 0 && score >= 60,
    score: Math.max(0, score),
    issues,
    warnings,
    similarity_score: 0, // å¾Œã§è¨ˆç®—
  };
}
```

---

## ğŸš€ Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰

```typescript
import crypto from 'crypto';

// å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: ãƒ¡ãƒ¢ãƒª â†’ KV â†’ D1 â†’ API
class EmbeddingCache {
  private memoryCache: Map<string, number[]> = new Map();
  private maxMemoryCacheSize = 100;

  async getEmbedding(text: string, env: Env): Promise<number[]> {
    const textHash = this.hashText(text);

    // Level 1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
    if (this.memoryCache.has(textHash)) {
      console.log('âœ… Embedding cache hit (memory)');
      return this.memoryCache.get(textHash)!;
    }

    // Level 2: KVï¼ˆé«˜é€Ÿï¼‰
    const kvKey = `embedding:${textHash}`;
    const kvCached = await env.KV.get(kvKey, 'json');
    if (kvCached) {
      console.log('âœ… Embedding cache hit (KV)');
      this.updateMemoryCache(textHash, kvCached);
      return kvCached;
    }

    // Level 3: D1ï¼ˆæ°¸ç¶šï¼‰
    const d1Cached = await env.DB.prepare(`
      SELECT embedding_json FROM embedding_cache WHERE text_hash = ?
    `).bind(textHash).first();

    if (d1Cached) {
      console.log('âœ… Embedding cache hit (D1)');
      const embedding = JSON.parse(d1Cached.embedding_json as string);
      
      // KVã¨ãƒ¡ãƒ¢ãƒªã«æ˜‡æ ¼
      await env.KV.put(kvKey, JSON.stringify(embedding), { expirationTtl: 3600 });
      this.updateMemoryCache(textHash, embedding);

      // ä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°ï¼ˆãƒˆãƒªã‚¬ãƒ¼ãŒè‡ªå‹•å®Ÿè¡Œï¼‰
      await env.DB.prepare(`
        UPDATE embedding_cache 
        SET last_used_at = CURRENT_TIMESTAMP, use_count = use_count + 1 
        WHERE text_hash = ?
      `).bind(textHash).run();

      return embedding;
    }

    // Level 4: APIå‘¼ã³å‡ºã—
    console.log('âŒ Embedding cache miss - calling API');
    const embedding = await this.fetchEmbeddingFromAPI(text, env);

    // å…¨ãƒ¬ãƒ™ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    await this.cacheEmbedding(textHash, embedding, env);

    return embedding;
  }

  private async fetchEmbeddingFromAPI(text: string, env: Env): Promise<number[]> {
    const response = await fetch('https://api.openai.com/v1/embeddings', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'text-embedding-3-small',
        input: text,
      }),
    });

    const data = await response.json();
    return data.data[0].embedding;
  }

  private async cacheEmbedding(textHash: string, embedding: number[], env: Env): Promise<void> {
    // ãƒ¡ãƒ¢ãƒª
    this.updateMemoryCache(textHash, embedding);

    // KVï¼ˆ1æ™‚é–“ï¼‰
    const kvKey = `embedding:${textHash}`;
    await env.KV.put(kvKey, JSON.stringify(embedding), { expirationTtl: 3600 });

    // D1ï¼ˆæ°¸ç¶šï¼‰
    await env.DB.prepare(`
      INSERT INTO embedding_cache (text_hash, model, embedding_json)
      VALUES (?, 'text-embedding-3-small', ?)
      ON CONFLICT(text_hash) DO UPDATE SET
        last_used_at = CURRENT_TIMESTAMP,
        use_count = use_count + 1
    `).bind(textHash, JSON.stringify(embedding)).run();
  }

  private updateMemoryCache(textHash: string, embedding: number[]): void {
    if (this.memoryCache.size >= this.maxMemoryCacheSize) {
      const firstKey = this.memoryCache.keys().next().value;
      this.memoryCache.delete(firstKey);
    }
    this.memoryCache.set(textHash, embedding);
  }

  private hashText(text: string): string {
    return crypto.createHash('sha256').update(text).digest('hex');
  }
}

// é¡ä¼¼åº¦è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
async function checkSimilarityWithCache(
  newQuestionText: string,
  analysisId: number,
  env: Env
): Promise<number> {
  const cache = new EmbeddingCache();

  // æ–°å•é¡Œã®Embedding
  const newEmbedding = await cache.getEmbedding(newQuestionText, env);

  // åŒã˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ—¢å­˜å•é¡Œã¨æ¯”è¼ƒ
  const analysis = await env.DB.prepare(`
    SELECT grade, section FROM question_analysis WHERE id = ?
  `).bind(analysisId).first();

  const existingQuestions = await env.DB.prepare(`
    SELECT question_text FROM generated_questions
    WHERE grade = ? AND section = ? AND review_status = 'approved'
    LIMIT 50
  `).bind(analysis.grade, analysis.section).all();

  let maxSimilarity = 0;
  for (const q of existingQuestions.results) {
    const existingEmbedding = await cache.getEmbedding(q.question_text as string, env);
    const similarity = cosineSimilarity(newEmbedding, existingEmbedding);
    
    if (similarity > maxSimilarity) {
      maxSimilarity = similarity;
    }
  }

  return maxSimilarity;
}

function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (magA * magB);
}
```

---

## â±ï¸ Durable Objectsï¼ˆé•·æ™‚é–“ã‚¿ã‚¹ã‚¯å‡¦ç†ï¼‰

```typescript
// Cron Triggerã®10ç§’åˆ¶é™ã‚’å›é¿
export class QuestionGeneratorDO {
  state: DurableObjectState;
  env: Env;

  constructor(state: DurableObjectState, env: Env) {
    this.state = state;
    this.env = env;
  }

  async fetch(request: Request) {
    const url = new URL(request.url);

    if (url.pathname === '/generate-batch') {
      // é•·æ™‚é–“ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹
      this.state.waitUntil(this.generateQuestionBatch());
      return new Response('Batch generation started', { status: 202 });
    }

    if (url.pathname === '/status') {
      const status = await this.state.storage.get('status');
      return new Response(JSON.stringify(status), {
        headers: { 'Content-Type': 'application/json' }
      });
    }

    return new Response('Not found', { status: 404 });
  }

  private async generateQuestionBatch() {
    await this.state.storage.put('status', { 
      state: 'running', 
      started: Date.now() 
    });

    try {
      // ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
      const poolStatus = await this.env.DB.prepare(`
        SELECT grade, section, COUNT(*) as count
        FROM generated_questions
        WHERE review_status = 'approved'
        GROUP BY grade, section
      `).all();

      const targetPoolSize = 50;
      let generated = 0;

      for (const row of poolStatus.results) {
        const { grade, section, count } = row;
        
        if ((count as number) < targetPoolSize) {
          const needed = targetPoolSize - (count as number);
          
          // åˆ†æIDã‚’å–å¾—
          const analyses = await this.env.DB.prepare(`
            SELECT id FROM question_analysis
            WHERE grade = ? AND section = ?
            ORDER BY RANDOM()
            LIMIT ?
          `).bind(grade, section, needed).all();

          // é †æ¬¡ç”Ÿæˆï¼ˆä¸¦åˆ—ã ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«å¼•ã£ã‹ã‹ã‚‹ï¼‰
          for (const analysis of analyses.results) {
            try {
              const draft = await generateQuestionFromAnalysis(analysis.id as number, this.env);
              await validateAndSave(draft, analysis.id as number, this.env);
              generated++;

              // é€²æ—ã‚’ä¿å­˜
              await this.state.storage.put('status', {
                state: 'running',
                generated,
                current: `${grade}-${section}`,
                updated: Date.now()
              });

              // ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆ1å•ã‚ãŸã‚Š2ç§’å¾…æ©Ÿï¼‰
              await new Promise(resolve => setTimeout(resolve, 2000));
            } catch (error) {
              console.error(`Failed to generate question:`, error);
            }
          }
        }
      }

      await this.state.storage.put('status', {
        state: 'completed',
        generated,
        completed: Date.now()
      });
    } catch (error) {
      await this.state.storage.put('status', {
        state: 'error',
        error: error.message,
        failed: Date.now()
      });
    }
  }
}

// wrangler.toml
// [[durable_objects.bindings]]
// name = "QUESTION_GENERATOR"
// class_name = "QuestionGeneratorDO"
// script_name = "eiken-system"

// Cron Triggerã‹ã‚‰ã®å‘¼ã³å‡ºã—
export default {
  async scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext) {
    // Durable Objectã«å§”è­²
    const id = env.QUESTION_GENERATOR.idFromName('main-generator');
    const stub = env.QUESTION_GENERATOR.get(id);
    
    ctx.waitUntil(stub.fetch('https://do.internal/generate-batch'));
  }
};
```

---

## ğŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

### JWTèªè¨¼å®Ÿè£…

```typescript
import jwt from '@tiptap/pm/lib/jwt';

interface JWTPayload {
  sub: string;        // student_id
  email: string;
  grade: string;
  iat: number;
  exp: number;
}

async function authenticateRequest(request: Request, env: Env): Promise<string | null> {
  const authHeader = request.headers.get('Authorization');
  if (!authHeader || !authHeader.startsWith('Bearer ')) {
    return null;
  }

  const token = authHeader.substring(7);

  try {
    const payload = jwt.verify(token, env.JWT_SECRET) as JWTPayload;
    
    // å­¦ç”Ÿãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãŒå­˜åœ¨ã—ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ç¢ºèª
    const profile = await env.DB.prepare(`
      SELECT id, account_status FROM student_profiles WHERE id = ?
    `).bind(payload.sub).first();

    if (!profile || profile.account_status !== 'active') {
      return null;
    }

    // æœ€çµ‚ãƒ­ã‚°ã‚¤ãƒ³æ™‚åˆ»ã‚’æ›´æ–°
    await env.DB.prepare(`
      UPDATE student_profiles SET last_login = CURRENT_TIMESTAMP WHERE id = ?
    `).bind(payload.sub).run();

    return payload.sub;
  } catch (error) {
    console.error('JWT verification failed:', error);
    return null;
  }
}

// ç›£æŸ»ãƒ­ã‚°è¨˜éŒ²
async function logAudit(
  studentId: string,
  action: string,
  resourceType: string | null,
  resourceId: string | null,
  request: Request,
  env: Env
): Promise<void> {
  const ipAddress = request.headers.get('CF-Connecting-IP') || 'unknown';
  const userAgent = request.headers.get('User-Agent') || 'unknown';

  await env.DB.prepare(`
    INSERT INTO audit_logs (
      student_id, action_type, resource_type, resource_id,
      ip_address, user_agent
    ) VALUES (?, ?, ?, ?, ?, ?)
  `).bind(studentId, action, resourceType, resourceId, ipAddress, userAgent).run();
}

// ä½¿ç”¨ä¾‹
export async function onRequestPost(context: { request: Request; env: Env }) {
  const studentId = await authenticateRequest(context.request, context.env);
  if (!studentId) {
    return new Response('Unauthorized', { status: 401 });
  }

  // ç›£æŸ»ãƒ­ã‚°
  await logAudit(studentId, 'question_solved', 'question', '123', context.request, context.env);

  // å‡¦ç†ç¶šè¡Œ...
}
```

---

## ğŸ“ˆ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ†ã‚¹ãƒˆè¨ˆç”»

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```typescript
// scripts/init-db.ts
async function initializeDatabase(env: Env) {
  console.log('ğŸš€ Initializing database...');

  // 1. å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’æœ‰åŠ¹åŒ–
  await env.DB.exec('PRAGMA foreign_keys = ON;');
  console.log('âœ… Foreign keys enabled');

  // 2. å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆä¸Šè¨˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å®Ÿè¡Œï¼‰
  // ... (çœç•¥)

  // 3. åˆæœŸã‚¿ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
  const initialTags = [
    { name: 'present_perfect', type: 'grammar', category: 'verb_tense' },
    { name: 'passive_voice', type: 'grammar', category: 'voice' },
    { name: 'relative_clauses', type: 'grammar', category: 'clause' },
    { name: 'CEFR-A2', type: 'vocabulary', category: 'level' },
    { name: 'CEFR-B1', type: 'vocabulary', category: 'level' },
    // ... more tags
  ];

  for (const tag of initialTags) {
    await env.DB.prepare(`
      INSERT OR IGNORE INTO tags (name, type, category)
      VALUES (?, ?, ?)
    `).bind(tag.name, tag.type, tag.category).run();
  }

  console.log(`âœ… Inserted ${initialTags.length} initial tags`);

  // 4. ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
  await env.DB.prepare(`
    INSERT OR IGNORE INTO student_profiles (id, email, display_name, target_grade)
    VALUES ('test-user-001', 'test@example.com', 'Test User', '2')
  `).run();

  console.log('âœ… Database initialized successfully');
}
```

### ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```typescript
// tests/database.test.ts
describe('Database Tests', () => {
  test('å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ãŒæœ‰åŠ¹', async () => {
    const result = await env.DB.prepare('PRAGMA foreign_keys').first();
    expect(result.foreign_keys).toBe(1);
  });

  test('ãƒˆãƒªã‚¬ãƒ¼ãŒå‹•ä½œï¼ˆupdated_atè‡ªå‹•æ›´æ–°ï¼‰', async () => {
    const before = await env.DB.prepare(`
      SELECT updated_at FROM student_profiles WHERE id = 'test-user-001'
    `).first();

    await new Promise(resolve => setTimeout(resolve, 1000));

    await env.DB.prepare(`
      UPDATE student_profiles SET display_name = 'Updated' WHERE id = 'test-user-001'
    `).run();

    const after = await env.DB.prepare(`
      SELECT updated_at FROM student_profiles WHERE id = 'test-user-001'
    `).first();

    expect(after.updated_at).not.toBe(before.updated_at);
  });

  test('ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãŒåŸå­æ€§ã‚’ä¿è¨¼', async () => {
    try {
      await env.DB.batch([
        env.DB.prepare(`INSERT INTO generated_questions (...) VALUES (...)`),
        env.DB.prepare(`INSERT INTO invalid_table (...) VALUES (...)`) // å¤±æ•—
      ]);
    } catch (error) {
      // å…¨ä½“ãŒãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã‚‹
    }

    // 1ã¤ç›®ã®INSERTã‚‚ä¿å­˜ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    const count = await env.DB.prepare(`SELECT COUNT(*) FROM generated_questions`).first();
    expect(count['COUNT(*)']).toBe(0);
  });

  test('Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå‹•ä½œ', async () => {
    const cache = new EmbeddingCache();
    const text = 'Test question text';

    // 1å›ç›®: APIå‘¼ã³å‡ºã—
    const start1 = Date.now();
    await cache.getEmbedding(text, env);
    const time1 = Date.now() - start1;

    // 2å›ç›®: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
    const start2 = Date.now();
    await cache.getEmbedding(text, env);
    const time2 = Date.now() - start2;

    expect(time2).toBeLessThan(time1 * 0.1); // 10å€ä»¥ä¸Šé«˜é€Ÿ
  });

  test('é¡ä¼¼åº¦æ¤œå‡ºãŒå‹•ä½œ', async () => {
    const similar = await checkSimilarityWithCache(
      'The cat sat on the mat',
      1,
      env
    );
    expect(similar).toBeGreaterThan(0.8);

    const different = await checkSimilarityWithCache(
      'Quantum physics is fascinating',
      1,
      env
    );
    expect(different).toBeLessThan(0.3);
  });
});
```

---

## ğŸ¯ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆV2ç‰ˆï¼‰

### Week 1-2: åŸºç›¤æ§‹ç¯‰
- [x] **è‘—ä½œæ¨©å®‰å…¨è¨­è¨ˆã®å®Ÿè£…**
  - question_analysis ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ä½¿ç”¨
  - éå»å•ã®åˆ†æé–¢æ•°
  - è‘—ä½œæ¨©ä¾µå®³é˜²æ­¢ã®ç¢ºèª
- [x] D1å¤–éƒ¨ã‚­ãƒ¼ãƒ»ãƒˆãƒªã‚¬ãƒ¼è¨­å®š
- [x] åŸºæœ¬APIï¼ˆèªè¨¼ã€ç›£æŸ»ãƒ­ã‚°ï¼‰
- [x] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼ˆåˆ†æçµæœã®ã¿ï¼‰

### Week 3-4: AIç”Ÿæˆã‚³ã‚¢
- [x] åˆ†æãƒ™ãƒ¼ã‚¹ã®å•é¡Œç”Ÿæˆ
- [x] 2æ®µéšæ¤œè¨¼ï¼ˆç”Ÿæˆ â†’ å“è³ªãƒã‚§ãƒƒã‚¯ï¼‰
- [x] Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
- [x] Durable Objectsçµ±åˆ

### Week 5-6: ãƒªã‚¹ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢
- [x] OpenAI TTSçµ±åˆ
- [x] R2éŸ³å£°ç®¡ç†
- [x] éŸ³å£°å†ç”ŸUI

### Week 7-8: å­¦ç¿’æ©Ÿèƒ½
- [x] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
- [x] å­¦ç¿’å±¥æ­´è¨˜éŒ²ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
- [x] å¼±ç‚¹åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- [x] SRSå¾©ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 9-10: æœ€é©åŒ–ãƒ»ãƒ†ã‚¹ãƒˆ
- [x] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [x] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
- [x] E2Eãƒ†ã‚¹ãƒˆ
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆ

---

## âœ… é‡è¦ãªæ±ºå®šäº‹é …ã¾ã¨ã‚ï¼ˆV2ï¼‰

### è‘—ä½œæ¨©
- âœ… **æœ€é‡è¦**: éå»å•ã®å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã¯ä¿å­˜ã—ãªã„
- âœ… åˆ†æçµæœã®ã¿DBä¿å­˜ï¼ˆgrammar_patterns, difficulty_scoreç­‰ï¼‰
- âœ… AIç”Ÿæˆå•é¡Œã®ã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›
- âœ… Embedding-basedé¡ä¼¼åº¦ç›£è¦–ï¼ˆé–¾å€¤85%ï¼‰

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- âœ… `PRAGMA foreign_keys = ON` èµ·å‹•æ™‚å®Ÿè¡Œ
- âœ… `env.DB.batch()` ã«ã‚ˆã‚‹ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
- âœ… è‡ªå‹•æ›´æ–°ãƒˆãƒªã‚¬ãƒ¼ï¼ˆupdated_atï¼‰
- âœ… UNIQUEåˆ¶ç´„ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- âœ… 3å±¤Embeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªâ†’KVâ†’D1ï¼‰
- âœ… Durable Objectsï¼ˆé•·æ™‚é–“ã‚¿ã‚¹ã‚¯ï¼‰
- âœ… ã‚­ãƒ¼ã‚»ãƒƒãƒˆãƒšãƒ¼ã‚¸ãƒ³ã‚°
- âœ… é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- âœ… JWTèªè¨¼
- âœ… student_profiles ãƒ†ãƒ¼ãƒ–ãƒ«
- âœ… audit_logsï¼ˆç›£æŸ»è¨¼è·¡ï¼‰
- âœ… å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§

---

## ğŸ‰ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **DBä½œæˆ**: `wrangler d1 create eiken-db-v2`
2. **ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: ä¸Šè¨˜V2ã‚¹ã‚­ãƒ¼ãƒã‚’å®Ÿè¡Œ
3. **éå»å•åˆ†æ**: 10å•ã®åˆ†æçµæœã‚’æŠ•å…¥ï¼ˆå•é¡Œæ–‡ã¯ä¿å­˜ã—ãªã„ï¼‰
4. **AIç”Ÿæˆãƒ†ã‚¹ãƒˆ**: åˆ†æã‹ã‚‰å•é¡Œç”Ÿæˆ
5. **è‘—ä½œæ¨©ç¢ºèª**: é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯å‹•ä½œç¢ºèª

**V2ã¯è‘—ä½œæ¨©å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸¡ç«‹ã—ãŸè¨­è¨ˆã§ã™ï¼** ğŸš€

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [Cloudflare D1 Documentation](https://developers.cloudflare.com/d1/)
- [D1 Best Practices](https://developers.cloudflare.com/d1/platform/best-practices/)
- [Durable Objects Guide](https://developers.cloudflare.com/workers/runtime-apis/durable-objects/)
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [SQLite Foreign Keys](https://www.sqlite.org/foreignkeys.html)
- [SQLite Triggers](https://www.sqlite.org/lang_createtrigger.html)
